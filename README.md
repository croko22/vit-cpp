# Transformer de traducciÃ³n

Este proyecto implementa un modelo **Transformer** utilizando Ãºnicamente **CUDA y C++**, sin bibliotecas de alto nivel como PyTorch o TensorFlow. La implementaciÃ³n es educativa y busca comprender los fundamentos del paper ["Attention is All You Need" (Vaswani et al., 2017)](https://arxiv.org/abs/1706.03762) a bajo nivel, acelerado en GPU.

## ðŸ§  Objetivo

Construir un modelo Transformer funcional en CUDA, desarrollando cada componente desde cero:
- Multi-Head Self-Attention
- Positional Encoding
- Feed Forward Networks
- NormalizaciÃ³n
- Mecanismos de codificador-decodificador
- TokenizaciÃ³n simple

## âš™ï¸ Estructura del Proyecto

```

marian-transformer-cuda/
â”œâ”€â”€ attention.cu        # Self-attention con matmul y softmax
â”œâ”€â”€ encoder.cu          # Encoder completo (por implementar)
â”œâ”€â”€ decoder.cu          # Decoder completo (por implementar)
â”œâ”€â”€ embeddings.cu       # Word + positional embeddings
â”œâ”€â”€ utils.cu/.h         # Funciones auxiliares (allocaciÃ³n, IO, etc)
â”œâ”€â”€ data/               # Datos de entrenamiento (WMT14 EN-FR preprocesado)
â”œâ”€â”€ main.cu             # Entry point, ejecuciÃ³n de training/inferencia
â””â”€â”€ README.md

````

## ðŸš€ CÃ³mo compilar y ejecutar

```bash
nvcc -arch=sm_60 -std=c++17 main.cu attention.cu encoder.cu decoder.cu -o transformer
./transformer
````

> Requiere: CUDA Toolkit â‰¥ 11.0, GPU NVIDIA con Compute Capability â‰¥ 6.0

## ðŸ“š Dataset

Se usarÃ¡ el dataset **WMT 2014 English-French**.
Por simplicidad, el pipeline usa una versiÃ³n tokenizada y numerizada (`.ids`). Se recomienda preprocesar con `sentencepiece` o similar en Python y exportar a texto plano.

## ðŸ§© Estado del Proyecto

* [x] MatMul en CUDA
* [x] Softmax fila a fila
* [x] CÃ¡lculo de atenciÃ³n (QÂ·K^T â†’ softmax â†’ Â·V)
* [ ] Multi-head attention
* [ ] Positional encoding
* [ ] Encoder y decoder stacks
* [ ] Entrenamiento simple (cross entropy)
* [ ] Inference con greedy decoding